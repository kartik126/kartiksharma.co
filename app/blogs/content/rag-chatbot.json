{
    "slug": "rag-chatbot-ollama-chromadb",
    "title": "Building a Local RAG Chatbot with Ollama and ChromaDB",
    "description": "How I built a Retrieval-Augmented Generation (RAG) chatbot that runs locally using Ollama for LLM inference and ChromaDB for vector search.",
    "date": "2025-05-15",
    "readTime": "8 min read",
    "content": "#\nRetrieval-Augmented Generation (RAG) is a powerful technique that combines the strengths of large language models (LLMs) with external knowledge sources. In this blog, I'll walk you through how I built a local RAG chatbot using [Ollama](https://ollama.com/) for LLM inference and [ChromaDB](https://www.trychroma.com/) as the vector database.\n\nYou can find the full backend code on **[GitHub](https://github.com/Beingkartik26/llama-bridge-py/tree/main/backend)**.\n\n## System Architecture\n\nThe overall flow of the system is illustrated below:\n\n![RAG Chatbot Architecture](/flow-chart.png)\n\n- **Data Ingestion:** Text or PDF data is split into chunks.\n- **Embedding:** Each chunk is embedded using a sentence transformer.\n- **Vector Storage:** The embeddings are stored in ChromaDB.\n- **Retrieval:** On user query, relevant chunks are retrieved from ChromaDB.\n- **LLM Augmentation:** The retrieved context is sent to Ollama for answer generation.\n\n## Key Components\n\n### 1. Chunking and Embedding\n\nI used the `nomic-embed-text` model for generating embeddings. Here's a snippet from [embed.py](https://github.com/Beingkartik26/llama-bridge-py/blob/main/backend/embed.py):\n\n```python\nfrom sentence_transformers import SentenceTransformer\n\ndef get_embeddings(texts):\n    model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1\")\n    return model.encode(texts)\n```\n\n### 2. Storing Embeddings in ChromaDB\n\nChromaDB is a lightweight, open-source vector database. Here's how I store the embeddings ([see full code](https://github.com/Beingkartik26/llama-bridge-py/blob/main/backend/embed.py)):\n\n```python\nimport chromadb\n\ndef store_embeddings(chunks, embeddings):\n    client = chromadb.Client()\n    collection = client.create_collection(\"my_collection\")\n    for chunk, embedding in zip(chunks, embeddings):\n        collection.add(\n            documents=[chunk],\n            embeddings=[embedding.tolist()],\n            metadatas=[{\"source\": \"my_data\"}]\n        )\n```\n\n### 3. Querying ChromaDB for Relevant Chunks\n\nWhen a user asks a question, I embed the query and retrieve the most relevant chunks ([see full code](https://github.com/Beingkartik26/llama-bridge-py/blob/main/backend/main.py)):\n\n```python\ndef retrieve_relevant_chunks(query, collection, model):\n    query_embedding = model.encode([query])[0]\n    results = collection.query(\n        query_embeddings=[query_embedding.tolist()],\n        n_results=3\n    )\n    return [doc for doc in results['documents'][0]]\n```\n\n### 4. Prompt Construction and LLM Inference\n\nThe retrieved context is formatted into a prompt and sent to Ollama for answer generation ([see full code](https://github.com/Beingkartik26/llama-bridge-py/blob/main/backend/main.py)):\n\n```python\ndef build_prompt(context, question):\n    return f\"Context: {context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\n\ndef get_llm_response(prompt):\n    # This function calls the Ollama API running locally\n    import requests\n    response = requests.post(\"http://localhost:11434/api/generate\", json={\"prompt\": prompt})\n    return response.json()[\"response\"]\n```\n\n### 5. Bringing It All Together\n\nThe main chat endpoint orchestrates the flow ([see full code](https://github.com/Beingkartik26/llama-bridge-py/blob/main/backend/main.py)):\n\n```python\n@app.post(\"/chat\")\ndef chat(query: str):\n    relevant_chunks = retrieve_relevant_chunks(query, collection, model)\n    context = \" \".join(relevant_chunks)\n    prompt = build_prompt(context, query)\n    answer = get_llm_response(prompt)\n    return {\"answer\": answer}\n```\n\n## Running Locally\n\n- Start Ollama with your preferred model (e.g., `llama3`).\n- Run the FastAPI backend: `uvicorn main:app --reload`\n- Interact with the chatbot via the `/chat` endpoint.\n\n## Conclusion\n\nThis project demonstrates how you can build a fully local, privacy-preserving RAG chatbot using open-source tools. The combination of ChromaDB and Ollama makes it easy to scale and customize for your own datasets.\n\n**Check out the full code on [GitHub](https://github.com/Beingkartik26/llama-bridge-py/tree/main/backend).**\n\n---\n\n*Feel free to reach out if you have questions or want to collaborate!*",
    "tags": ["RAG", "Ollama", "ChromaDB", "LLM", "Chatbot", "Python"]
} 